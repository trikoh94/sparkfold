import streamlit as st
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import numpy as np
from scipy import stats
from datetime import datetime, timedelta
from sklearn.cluster import KMeans
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout
import joblib
import streamlit.components.v1 as components

from data_collector import DataCollector
from data_analyzer import DataAnalyzer

def create_metric_card(title, value, delta=None, delta_color="normal"):
    """Î©îÌä∏Î¶≠ Ïπ¥Îìú ÏÉùÏÑ±"""
    st.metric(
        label=title,
        value=value,
        delta=delta,
        delta_color=delta_color
    )

def check_data_quality(df, name):
    """Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ï≤¥ÌÅ¨"""
    if df.empty:
        return {
            'status': '‚ö†Ô∏è',
            'message': f'No {name} data available'
        }
    
    total_rows = len(df)
    null_counts = df.isnull().sum()
    null_percentages = (null_counts / total_rows * 100).round(2)
    
    return {
        'status': '‚úÖ' if null_percentages.max() < 10 else '‚ö†Ô∏è',
        'message': f'{name} data available ({total_rows:,} rows)',
        'null_percentages': null_percentages[null_percentages > 0]
    }

def create_sequences(data, seq_length):
    """ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Î•º ÏãúÌÄÄÏä§Î°ú Î≥ÄÌôò"""
    X, y = [], []
    for i in range(len(data) - seq_length):
        X.append(data[i:(i + seq_length)])
        y.append(data[i + seq_length])
    return np.array(X), np.array(y)

def build_lstm_model(seq_length, n_features):
    """LSTM Î™®Îç∏ ÏÉùÏÑ±"""
    model = Sequential([
        LSTM(50, activation='relu', input_shape=(seq_length, n_features), return_sequences=True),
        Dropout(0.2),
        LSTM(50, activation='relu'),
        Dropout(0.2),
        Dense(1)
    ])
    model.compile(optimizer='adam', loss='mse')
    return model

def predict_time_series(events_df, target_column='event_count', days_to_predict=30):
    """ÏãúÍ≥ÑÏó¥ ÏòàÏ∏° ÏàòÌñâ"""
    # ÏùºÎ≥Ñ Îç∞Ïù¥ÌÑ∞ ÏßëÍ≥Ñ
    daily_data = events_df.groupby(pd.to_datetime(events_df['timestamp']).dt.date).size().reset_index(name=target_column)
    daily_data.set_index('timestamp', inplace=True)
    
    # Îç∞Ïù¥ÌÑ∞ Ï†ïÍ∑úÌôî
    scaler = MinMaxScaler()
    scaled_data = scaler.fit_transform(daily_data[[target_column]])
    
    # ÏãúÌÄÄÏä§ ÏÉùÏÑ±
    seq_length = 7  # 7Ïùº Îç∞Ïù¥ÌÑ∞Î°ú Îã§Ïùå ÎÇ† ÏòàÏ∏°
    X, y = create_sequences(scaled_data, seq_length)
    
    # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
    train_size = int(len(X) * 0.8)
    X_train, X_test = X[:train_size], X[train_size:]
    y_train, y_test = y[:train_size], y[train_size:]
    
    # Î™®Îç∏ ÌïôÏäµ
    model = build_lstm_model(seq_length, 1)
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    
    # ÎØ∏Îûò ÏòàÏ∏°
    last_sequence = scaled_data[-seq_length:]
    future_predictions = []
    
    for _ in range(days_to_predict):
        next_pred = model.predict(last_sequence.reshape(1, seq_length, 1), verbose=0)
        future_predictions.append(next_pred[0, 0])
        last_sequence = np.roll(last_sequence, -1)
        last_sequence[-1] = next_pred
    
    # ÏòàÏ∏°Í∞í Ïó≠Ï†ïÍ∑úÌôî
    future_predictions = scaler.inverse_transform(np.array(future_predictions).reshape(-1, 1))
    
    # ÏòàÏ∏° ÎÇ†Ïßú ÏÉùÏÑ±
    last_date = daily_data.index[-1]
    future_dates = [last_date + timedelta(days=i+1) for i in range(days_to_predict)]
    
    return daily_data, future_dates, future_predictions

def generate_sample_data():
    """ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
    np.random.seed(42)
    sample_size = 5000
    dates = pd.date_range(start='2024-01-01', end='2024-03-01', freq='D')
    
    # Í∏∞Î≥∏ Ïù¥Î≤§Ìä∏ Îç∞Ïù¥ÌÑ∞
    events_df = pd.DataFrame({
        'timestamp': np.random.choice(dates, sample_size),
        'user_id': np.random.randint(1, 200, sample_size),
        'event_type': np.random.choice([
            'idea_created', 'project_created', 'portfolio_created', 
            'idea_viewed', 'idea_shared', 'project_updated',
            'portfolio_shared', 'comment_added'
        ], sample_size),
        'session_duration': np.random.randint(60, 7200, sample_size),
        'idea_complexity': np.random.randint(1, 10, sample_size),
        'user_experience': np.random.randint(1, 100, sample_size),
        'interaction_count': np.random.randint(1, 50, sample_size),
    })
    
    # ÏãúÍ≥ÑÏó¥ Ìå®ÌÑ¥ Ï∂îÍ∞Ä
    base_activity = 100  # Í∏∞Î≥∏ ÌôúÎèôÎüâ
    weekly_pattern = np.sin(np.arange(len(dates)) * 2 * np.pi / 7) * 50  # Ï£ºÍ∞Ñ Ìå®ÌÑ¥
    trend = np.linspace(0, 50, len(dates))  # ÏÉÅÏäπ Ìä∏Î†åÎìú
    noise = np.random.normal(0, 10, len(dates))  # ÎÖ∏Ïù¥Ï¶à
    
    daily_activity = base_activity + weekly_pattern + trend + noise
    daily_activity = np.maximum(daily_activity, 0)  # ÏùåÏàò Î∞©ÏßÄ
    
    # Ïù¥Î≤§Ìä∏ Ïàò Ï°∞Ï†ï
    for i, date in enumerate(dates):
        date_events = events_df[events_df['timestamp'] == date]
        if len(date_events) > 0:
            target_count = int(daily_activity[i])
            if len(date_events) > target_count:
                events_df = events_df[~((events_df['timestamp'] == date) & 
                                     (events_df.index.isin(date_events.index[target_count:])))]
    
    # ÌîÑÎ°úÏ†ùÌä∏ Îç∞Ïù¥ÌÑ∞
    projects_df = pd.DataFrame({
        'project_id': range(1, 501),
        'user_id': np.random.randint(1, 200, 500),
        'created_at': np.random.choice(dates, 500),
        'completion_status': np.random.choice(['completed', 'in_progress', 'abandoned'], 500),
        'project_duration': np.random.randint(1, 90, 500),
        'team_size': np.random.randint(1, 8, 500),
        'complexity_score': np.random.randint(1, 10, 500),
        'milestone_count': np.random.randint(1, 10, 500),
    })
    
    # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ Îç∞Ïù¥ÌÑ∞
    portfolios_df = pd.DataFrame({
        'portfolio_id': range(1, 301),
        'user_id': np.random.randint(1, 200, 300),
        'created_at': np.random.choice(dates, 300),
        'type': np.random.choice(['personal', 'professional', 'academic', 'startup', 'research'], 300),
        'is_shared': np.random.choice([True, False], 300),
        'view_count': np.random.randint(0, 5000, 300),
        'like_count': np.random.randint(0, 1000, 300),
        'comment_count': np.random.randint(0, 500, 300),
        'last_updated': np.random.choice(dates, 300),
    })
    
    return events_df, projects_df, portfolios_df

def perform_user_clustering(events_df):
    """ÏÇ¨Ïö©Ïûê ÌñâÎèô ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ"""
    # ÏÇ¨Ïö©ÏûêÎ≥Ñ ÌäπÏÑ± Ï∂îÏ∂ú
    user_features = events_df.groupby('user_id').agg({
        'session_duration': 'mean',
        'idea_complexity': 'mean',
        'user_experience': 'mean'
    }).reset_index()
    
    # ÌäπÏÑ± Ïä§ÏºÄÏùºÎßÅ
    scaler = StandardScaler()
    features_scaled = scaler.fit_transform(user_features.drop('user_id', axis=1))
    
    # K-means ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ
    kmeans = KMeans(n_clusters=3, random_state=42)
    user_features['cluster'] = kmeans.fit_predict(features_scaled)
    
    return user_features

def predict_conversion(events_df, projects_df):
    """ÏïÑÏù¥ÎîîÏñ¥-ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÌôò ÏòàÏ∏°"""
    # ÌäπÏÑ± ÏóîÏßÄÎãàÏñ¥ÎßÅ
    user_events = events_df.groupby('user_id').agg({
        'session_duration': ['mean', 'sum'],
        'idea_complexity': 'mean',
        'user_experience': 'mean'
    }).reset_index()
    
    user_events.columns = ['user_id', 'avg_session_duration', 'total_session_duration', 
                          'avg_idea_complexity', 'user_experience']
    
    # Ï†ÑÌôò Ïó¨Î∂Ä Î†àÏù¥Î∏î ÏÉùÏÑ±
    converted_users = projects_df['user_id'].unique()
    user_events['converted'] = user_events['user_id'].isin(converted_users).astype(int)
    
    # Î™®Îç∏ ÌïôÏäµ
    X = user_events.drop(['user_id', 'converted'], axis=1)
    y = user_events['converted']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestClassifier(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    return model, X_test, y_test

def predict_engagement(events_df):
    """ÏÇ¨Ïö©Ïûê Ï∞∏Ïó¨ÎèÑ ÏòàÏ∏°"""
    # ÏÇ¨Ïö©ÏûêÎ≥Ñ Ï∞∏Ïó¨ÎèÑ Í≥ÑÏÇ∞
    user_engagement = events_df.groupby('user_id').agg({
        'session_duration': 'sum',
        'idea_complexity': 'mean',
        'user_experience': 'mean'
    }).reset_index()
    
    # Ï∞∏Ïó¨ÎèÑ Ï†êÏàò Í≥ÑÏÇ∞ (Ïòà: ÏÑ∏ÏÖò ÏãúÍ∞ÑÏùò Í∞ÄÏ§ë ÌèâÍ∑†)
    user_engagement['engagement_score'] = (
        user_engagement['session_duration'] * 0.5 +
        user_engagement['idea_complexity'] * 0.3 +
        user_engagement['user_experience'] * 0.2
    )
    
    # Î™®Îç∏ ÌïôÏäµ
    X = user_engagement.drop(['user_id', 'engagement_score'], axis=1)
    y = user_engagement['engagement_score']
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
    
    model = RandomForestRegressor(n_estimators=100, random_state=42)
    model.fit(X_train, y_train)
    
    return model, X_test, y_test

def main():
    st.set_page_config(
        page_title="IdeaHub Analytics Dashboard",
        page_icon="üìä",
        layout="wide"
    )
    
    st.title("IdeaHub Analytics Dashboard")
    
    # Architecture Image
    st.header("System Architecture")
    st.image("assets/arch.png", use_column_width=True)
    
    # Îç∞Ïù¥ÌÑ∞ Î°úÎìú
    try:
        collector = DataCollector()
        events_df = collector.collect_events()
        portfolios_df = collector.collect_portfolio_data()
        projects_df = collector.collect_project_data()
    except Exception as e:
        st.warning("Firebase connection failed. Using sample data instead.")
        events_df, projects_df, portfolios_df = generate_sample_data()
    
    # Îç∞Ïù¥ÌÑ∞ ÌíàÏßà Ï≤¥ÌÅ¨ ÏÑπÏÖò
    st.sidebar.header("Data Quality Check")
    events_quality = check_data_quality(events_df, "Events")
    portfolios_quality = check_data_quality(portfolios_df, "Portfolios")
    projects_quality = check_data_quality(projects_df, "Projects")
    
    st.sidebar.subheader("Data Status")
    st.sidebar.write(f"{events_quality['status']} {events_quality['message']}")
    st.sidebar.write(f"{portfolios_quality['status']} {portfolios_quality['message']}")
    st.sidebar.write(f"{projects_quality['status']} {projects_quality['message']}")
    
    # Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Í∏∞Í∞Ñ ÌëúÏãú
    if not events_df.empty and 'timestamp' in events_df.columns:
        start_date = pd.to_datetime(events_df['timestamp']).min().date()
        end_date = pd.to_datetime(events_df['timestamp']).max().date()
        st.sidebar.subheader("Data Collection Period")
        st.sidebar.write(f"From: {start_date}")
        st.sidebar.write(f"To: {end_date}")
        st.sidebar.write(f"Total Days: {(end_date - start_date).days + 1}")
    
    # ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± ÏòµÏÖò
    if st.sidebar.checkbox("Regenerate Sample Data"):
        st.sidebar.warning("This will regenerate sample data for demonstration purposes")
        if st.sidebar.button("Generate"):
            events_df, projects_df, portfolios_df = generate_sample_data()
            st.sidebar.success("Sample data regenerated!")
    
    # Îç∞Ïù¥ÌÑ∞Í∞Ä ÎπÑÏñ¥ÏûàÎäî Í≤ΩÏö∞ Ï≤òÎ¶¨
    if events_df.empty:
        st.warning("No event data available. Please check Firebase connection and data.")
        return
    
    # Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî
    analyzer = DataAnalyzer(events_df)
    
    # ÌÉ≠ ÏÉùÏÑ±
    tab1, tab2, tab3, tab4, tab5, tab6 = st.tabs([
        "Overview", "Mathematical Analysis", "User Behavior", 
        "Growth Metrics", "Machine Learning", "Architecture"
    ])
    
    with tab1:
        st.header('Key Performance Indicators')
        
        # KPI Î©îÌä∏Î¶≠Ïä§
        col1, col2, col3, col4 = st.columns(4)
        
        with col1:
            total_users = len(events_df['user_id'].unique()) if 'user_id' in events_df.columns else 0
            create_metric_card("Total Users", f"{total_users:,}")
        
        with col2:
            total_ideas = len(events_df[events_df['event_type'] == 'idea_created']) if 'event_type' in events_df.columns else 0
            create_metric_card("Total Ideas", f"{total_ideas:,}")
        
        with col3:
            total_projects = len(projects_df) if not projects_df.empty else 0
            create_metric_card("Total Projects", f"{total_projects:,}")
        
        with col4:
            total_portfolios = len(portfolios_df) if not portfolios_df.empty else 0
            create_metric_card("Total Portfolios", f"{total_portfolios:,}")
        
        # Ï†ÑÌôòÏú® Ï∞®Ìä∏
        st.subheader('Conversion Funnel')
        funnel_data = {
            'Stage': ['Ideas', 'Projects', 'Portfolios', 'Shared'],
            'Count': [
                len(events_df[events_df['event_type'] == 'idea_created']) if 'event_type' in events_df.columns else 0,
                len(projects_df) if not projects_df.empty else 0,
                len(portfolios_df) if not portfolios_df.empty else 0,
                len(portfolios_df[portfolios_df['is_shared'] == True]) if not portfolios_df.empty and 'is_shared' in portfolios_df.columns else 0
            ]
        }
        funnel_df = pd.DataFrame(funnel_data)
        
        fig_funnel = go.Figure(go.Funnel(
            y=funnel_df['Stage'],
            x=funnel_df['Count'],
            textinfo="value+percent initial"
        ))
        fig_funnel.update_layout(title='Conversion Funnel Analysis')
        st.plotly_chart(fig_funnel, use_container_width=True)
        
        # ÏãúÍ∞ÑÎåÄÎ≥Ñ ÌôúÎèô Î∂ÑÏÑù
        st.subheader('Activity by Time')
        if 'timestamp' in events_df.columns:
            events_df['hour'] = pd.to_datetime(events_df['timestamp']).dt.hour
            hourly_activity = events_df.groupby('hour').size().reset_index(name='count')
            
            fig_hourly = px.bar(
                hourly_activity,
                x='hour',
                y='count',
                title='Hourly Activity Distribution',
                labels={'hour': 'Hour of Day', 'count': 'Number of Events'}
            )
            st.plotly_chart(fig_hourly, use_container_width=True)
        else:
            st.warning("Timestamp data not available for hourly analysis")
        
        # Ìè¨Ìä∏Ìè¥Î¶¨Ïò§ ÌÉÄÏûÖ Î∂ÑÌè¨
        st.subheader('Portfolio Type Distribution')
        if not portfolios_df.empty and 'type' in portfolios_df.columns:
            portfolio_types = portfolios_df['type'].value_counts()
            fig_portfolio = px.pie(
                values=portfolio_types.values,
                names=portfolio_types.index,
                title='Portfolio Types'
            )
            st.plotly_chart(fig_portfolio, use_container_width=True)
        else:
            st.warning("Portfolio type data not available")
    
    with tab2:
        st.header('Mathematical Analysis')
        
        if not events_df.empty:
            # 1. ÌôïÎ•†Î°†Ï†Å Î∂ÑÏÑù
            st.subheader("1. Probability Analysis")
            col1, col2 = st.columns(2)
            
            with col1:
                # Ïù¥ÌÉàÎ•† (Î≤†Ïù¥ÏßÄÏïà ÏóÖÎç∞Ïù¥Ìä∏)
                bounce_rate = analyzer.get_bounce_rate()
                create_metric_card("Bounce Rate", f"{bounce_rate:.2%}")
                
                # Ï†ÑÌôòÏú® (Ï°∞Í±¥Î∂Ä ÌôïÎ•†)
                conversion_rate = analyzer.get_conversion_rate()
                create_metric_card("Conversion Rate", f"{conversion_rate:.2%}")
            
            with col2:
                # ÌîÑÎ°úÏ†ùÌä∏ ÏôÑÎ£åÏú® (Î≤†Î•¥ÎàÑÏù¥ Î∂ÑÌè¨)
                completion_rate = analyzer.get_completion_rate()
                create_metric_card("Project Completion Rate", f"{completion_rate:.2%}")
                
                # ÏïÑÏù¥ÎîîÏñ¥-ÌîÑÎ°úÏ†ùÌä∏ Ï†ÑÌôòÏú®
                idea_to_project_rate = analyzer.get_idea_to_project_rate()
                create_metric_card("Idea to Project Rate", f"{idea_to_project_rate:.2%}")
            
            # 2. ÏãúÍ≥ÑÏó¥ Î∂ÑÏÑù
            st.subheader("2. Time Series Analysis")
            
            # ÏùºÎ≥Ñ ÌôúÎèô Ï∂îÏù¥
            events_df['date'] = pd.to_datetime(events_df['timestamp']).dt.date
            daily_activity = events_df.groupby('date').size().reset_index(name='count')
            
            fig_daily = px.line(
                daily_activity,
                x='date',
                y='count',
                title='Daily Activity Trend',
                labels={'date': 'Date', 'count': 'Number of Events'}
            )
            st.plotly_chart(fig_daily, use_container_width=True)
            
            # 3. ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ Î∂ÑÏÑù
            st.subheader("3. Correlation Analysis")
            
            if not portfolios_df.empty:
                # Ï°∞ÌöåÏàòÏôÄ Ï¢ãÏïÑÏöî ÏàòÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
                fig_correlation = px.scatter(
                    portfolios_df,
                    x='view_count',
                    y='like_count',
                    color='type',
                    title='Correlation: Views vs Likes by Portfolio Type',
                    labels={'view_count': 'Number of Views', 'like_count': 'Number of Likes'}
                )
                st.plotly_chart(fig_correlation, use_container_width=True)
                
                # ÎåìÍ∏Ä ÏàòÏôÄ Ï¢ãÏïÑÏöî ÏàòÏùò ÏÉÅÍ¥ÄÍ¥ÄÍ≥Ñ
                fig_correlation2 = px.scatter(
                    portfolios_df,
                    x='comment_count',
                    y='like_count',
                    color='type',
                    title='Correlation: Comments vs Likes by Portfolio Type',
                    labels={'comment_count': 'Number of Comments', 'like_count': 'Number of Likes'}
                )
                st.plotly_chart(fig_correlation2, use_container_width=True)
    
    with tab3:
        st.header('User Behavior Analysis')
        
        if not events_df.empty:
            # 1. ÏÇ¨Ïö©Ïûê ÌñâÎèô ÌùêÎ¶Ñ
            st.subheader("1. User Journey Analysis")
            
            # Ïù¥Î≤§Ìä∏ ÏãúÌÄÄÏä§ Î∂ÑÏÑù
            event_sequences = analyzer.get_event_sequences()
            if not event_sequences.empty:
                fig_sequences = px.sunburst(
                    event_sequences,
                    path=['sequence'],
                    values='count',
                    title='Top User Event Sequences'
                )
                st.plotly_chart(fig_sequences, use_container_width=True)
            
            # 2. ÏÇ¨Ïö©Ïûê ÏÑ∏Í∑∏Î®ºÌä∏
            st.subheader("2. User Segments")
            
            # ÌôúÎèô ÏàòÏ§ÄÎ≥Ñ ÏÇ¨Ïö©Ïûê Î∂ÑÎ•ò
            user_activity = events_df.groupby('user_id').size().reset_index(name='activity_count')
            user_activity['segment'] = pd.qcut(
                user_activity['activity_count'],
                q=4,
                labels=['Low', 'Medium', 'High', 'Very High']
            )
            
            fig_segments = px.pie(
                user_activity,
                names='segment',
                title='User Activity Segments'
            )
            st.plotly_chart(fig_segments, use_container_width=True)
            
            # 3. ÏÇ¨Ïö©Ïûê ÌñâÎèô Ìå®ÌÑ¥
            st.subheader("3. User Behavior Patterns")
            
            # Ïù¥Î≤§Ìä∏ ÌÉÄÏûÖÎ≥Ñ Î∂ÑÌè¨
            event_types = events_df['event_type'].value_counts()
            fig_events = px.bar(
                x=event_types.index,
                y=event_types.values,
                title='Event Type Distribution',
                labels={'x': 'Event Type', 'y': 'Count'}
            )
            st.plotly_chart(fig_events, use_container_width=True)
    
    with tab4:
        st.header('Growth Metrics')
        
        if not events_df.empty:
            # 1. ÏÑ±Ïû• ÏßÄÌëú
            st.subheader("1. Growth Metrics")
            
            # ÏùºÎ≥Ñ Ïã†Í∑ú ÏÇ¨Ïö©Ïûê
            events_df['date'] = pd.to_datetime(events_df['timestamp']).dt.date
            new_users = events_df.groupby('date')['user_id'].nunique().reset_index(name='new_users')
            
            fig_new_users = px.line(
                new_users,
                x='date',
                y='new_users',
                title='Daily New Users',
                labels={'date': 'Date', 'new_users': 'Number of New Users'}
            )
            st.plotly_chart(fig_new_users, use_container_width=True)
            
            # 2. Ï∞∏Ïó¨ÎèÑ ÏßÄÌëú
            st.subheader("2. Engagement Metrics")
            
            # ÏÇ¨Ïö©ÏûêÎãπ ÌèâÍ∑† Ïù¥Î≤§Ìä∏ Ïàò
            user_engagement = events_df.groupby('user_id').size().reset_index(name='event_count')
            avg_events = user_engagement['event_count'].mean()
            
            create_metric_card(
                "Average Events per User",
                f"{avg_events:.1f}",
                delta=f"{avg_events - user_engagement['event_count'].median():.1f}",
                delta_color="normal"
            )
            
            # 3. Ï†ÑÌôòÏú® Ï∂îÏù¥
            st.subheader("3. Conversion Rate Trends")
            
            # ÏùºÎ≥Ñ Ï†ÑÌôòÏú® Í≥ÑÏÇ∞
            daily_conversion = events_df.groupby('date').apply(
                lambda x: len(x[x['event_type'] == 'project_created']) / len(x[x['event_type'] == 'idea_created'])
                if len(x[x['event_type'] == 'idea_created']) > 0 else 0
            ).reset_index(name='conversion_rate')
            
            fig_conversion = px.line(
                daily_conversion,
                x='date',
                y='conversion_rate',
                title='Daily Conversion Rate Trend',
                labels={'date': 'Date', 'conversion_rate': 'Conversion Rate'}
            )
            st.plotly_chart(fig_conversion, use_container_width=True)
    
    with tab5:
        st.header('Machine Learning Analysis')
        
        if not events_df.empty:
            # 1. ÏÇ¨Ïö©Ïûê ÌÅ¥Îü¨Ïä§ÌÑ∞ÎßÅ
            st.subheader("1. User Behavior Clustering")
            user_clusters = perform_user_clustering(events_df)
            
            fig_clusters = px.scatter_3d(
                user_clusters,
                x='session_duration',
                y='idea_complexity',
                z='user_experience',
                color='cluster',
                title='User Behavior Clusters'
            )
            st.plotly_chart(fig_clusters, use_container_width=True)
            
            # 2. Ï†ÑÌôò ÏòàÏ∏°
            st.subheader("2. Conversion Prediction")
            conversion_model, X_test, y_test = predict_conversion(events_df, projects_df)
            
            # Î™®Îç∏ ÏÑ±Îä• ÌëúÏãú
            accuracy = conversion_model.score(X_test, y_test)
            create_metric_card("Conversion Prediction Accuracy", f"{accuracy:.2%}")
            
            # ÌäπÏÑ± Ï§ëÏöîÎèÑ ÏãúÍ∞ÅÌôî
            feature_importance = pd.DataFrame({
                'feature': X_test.columns,
                'importance': conversion_model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            fig_importance = px.bar(
                feature_importance,
                x='feature',
                y='importance',
                title='Feature Importance for Conversion Prediction'
            )
            st.plotly_chart(fig_importance, use_container_width=True)
            
            # 3. Ï∞∏Ïó¨ÎèÑ ÏòàÏ∏°
            st.subheader("3. Engagement Prediction")
            engagement_model, X_test, y_test = predict_engagement(events_df)
            
            # Î™®Îç∏ ÏÑ±Îä• ÌëúÏãú
            r2_score = engagement_model.score(X_test, y_test)
            create_metric_card("Engagement Prediction R¬≤ Score", f"{r2_score:.2f}")
            
            # ÏòàÏ∏°Í∞í vs Ïã§Ï†úÍ∞í ÏãúÍ∞ÅÌôî
            y_pred = engagement_model.predict(X_test)
            fig_engagement = px.scatter(
                x=y_test,
                y=y_pred,
                title='Predicted vs Actual Engagement Scores',
                labels={'x': 'Actual', 'y': 'Predicted'}
            )
            fig_engagement.add_scatter(
                x=[y_test.min(), y_test.max()],
                y=[y_test.min(), y_test.max()],
                mode='lines',
                name='Perfect Prediction'
            )
            st.plotly_chart(fig_engagement, use_container_width=True)
            
            # 4. ÏãúÍ≥ÑÏó¥ ÏòàÏ∏°
            st.subheader("4. Time Series Prediction")
            
            # ÏòàÏ∏° ÏàòÌñâ
            daily_data, future_dates, future_predictions = predict_time_series(events_df)
            
            # ÏãúÍ∞ÅÌôî
            fig_forecast = go.Figure()
            
            # Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞
            fig_forecast.add_trace(go.Scatter(
                x=daily_data.index,
                y=daily_data['event_count'],
                name='Actual',
                line=dict(color='blue')
            ))
            
            # ÏòàÏ∏° Îç∞Ïù¥ÌÑ∞
            fig_forecast.add_trace(go.Scatter(
                x=future_dates,
                y=future_predictions.flatten(),
                name='Forecast',
                line=dict(color='red', dash='dash')
            ))
            
            fig_forecast.update_layout(
                title='Daily Activity Forecast',
                xaxis_title='Date',
                yaxis_title='Number of Events',
                showlegend=True
            )
            
            st.plotly_chart(fig_forecast, use_container_width=True)
            
            # ÏòàÏ∏° Í≤∞Í≥º ÏöîÏïΩ
            st.subheader("Forecast Summary")
            col1, col2 = st.columns(2)
            
            with col1:
                avg_current = daily_data['event_count'].mean()
                avg_forecast = np.mean(future_predictions)
                growth_rate = ((avg_forecast - avg_current) / avg_current) * 100
                create_metric_card(
                    "Expected Growth Rate",
                    f"{growth_rate:.1f}%",
                    delta=f"{growth_rate:.1f}%",
                    delta_color="normal" if growth_rate > 0 else "inverse"
                )
            
            with col2:
                max_forecast = np.max(future_predictions)
                create_metric_card(
                    "Peak Activity Forecast",
                    f"{int(max_forecast)}",
                    delta=f"{int(max_forecast - avg_current)}",
                    delta_color="normal"
                )

    with tab6:
        st.header('System Architecture')
        
        # Data Flow Architecture
        st.subheader('Data Flow Architecture')
        data_flow_diagram = """
        graph TD
            A[Firebase Realtime DB] -->|Real-time Stream| B[Data Collector]
            A -->|Batch Load| B
            B -->|Processed Data| C[Data Storage]
            C -->|Analysis Ready| D[Analytics Engine]
            D -->|Insights| E[Dashboard]
            D -->|Predictions| E
            D -->|Visualizations| E
        """
        components.html(f"""
            <div class="mermaid">
            {data_flow_diagram}
            </div>
            <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
            <script>mermaid.initialize({{startOnLoad:true}});</script>
        """, height=400)
        
        # Storage Architecture
        st.subheader('Storage Architecture')
        storage_diagram = """
        graph LR
            A[Firebase Realtime DB] -->|Primary Storage| B[Data Layer]
            B -->|Cache| C[Local Cache]
            B -->|Process| D[Analysis Storage]
            D -->|Results| E[Visualization Cache]
        """
        components.html(f"""
            <div class="mermaid">
            {storage_diagram}
            </div>
            <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
            <script>mermaid.initialize({{startOnLoad:true}});</script>
        """, height=300)
        
        # ETL Process Flow
        st.subheader('ETL Process Flow')
        etl_diagram = """
        graph TD
            A[Extract] -->|Raw Data| B[Transform]
            B -->|Cleaned Data| C[Load]
            C -->|Processed Data| D[Analysis]
            D -->|Results| E[Visualization]
            
            subgraph Extract
            A1[Firebase Events] --> A
            A2[Firebase Projects] --> A
            A3[Firebase Portfolios] --> A
            end
            
            subgraph Transform
            B1[Data Cleaning] --> B
            B2[Feature Engineering] --> B
            B3[Data Normalization] --> B
            end
            
            subgraph Load
            C1[Quality Check] --> C
            C2[Data Validation] --> C
            C3[Storage Optimization] --> C
            end
        """
        components.html(f"""
            <div class="mermaid">
            {etl_diagram}
            </div>
            <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
            <script>mermaid.initialize({{startOnLoad:true}});</script>
        """, height=500)
        
        # Analytics Pipeline
        st.subheader('Analytics Pipeline')
        analytics_diagram = """
        graph TD
            A[Data Input] -->|Process| B[Analytics Engine]
            B -->|Descriptive| C[KPI Analysis]
            B -->|Diagnostic| D[Pattern Analysis]
            B -->|Predictive| E[ML Models]
            B -->|Prescriptive| F[Recommendations]
            
            C -->|Results| G[Dashboard]
            D -->|Results| G
            E -->|Results| G
            F -->|Results| G
        """
        components.html(f"""
            <div class="mermaid">
            {analytics_diagram}
            </div>
            <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
            <script>mermaid.initialize({{startOnLoad:true}});</script>
        """, height=400)
        
        # Component Architecture
        st.subheader('Component Architecture')
        component_diagram = """
        graph TD
            A[Dashboard.py] -->|Uses| B[DataCollector]
            A -->|Uses| C[DataAnalyzer]
            B -->|Collects| D[Firebase Data]
            C -->|Analyzes| E[Processed Data]
            A -->|Displays| F[Streamlit UI]
            F -->|Shows| G[Visualizations]
            F -->|Shows| H[Metrics]
            F -->|Shows| I[Predictions]
        """
        components.html(f"""
            <div class="mermaid">
            {component_diagram}
            </div>
            <script src="https://cdn.jsdelivr.net/npm/mermaid/dist/mermaid.min.js"></script>
            <script>mermaid.initialize({{startOnLoad:true}});</script>
        """, height=400)

if __name__ == "__main__":
    main() 